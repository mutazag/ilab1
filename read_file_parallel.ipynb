{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# # https://www.tjansson.dk/2018/04/parallel-processing-pandas-dataframes/\n",
    "from utils.data import Data\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_split(file, col_names=None, header=None):\n",
    "    \"\"\"read a single csv file from a collection of file splits, to use this\n",
    "    correctly all files should have the same structure, ideally should be\n",
    "    generated using the split command\n",
    "\n",
    "    Arguments:\n",
    "        file {string} -- file path for the part to read\n",
    "\n",
    "    Keyword Arguments:\n",
    "        col_names {list} -- names of columns, best used with later split reads to\n",
    "                            apply header (default: {None})\n",
    "        header {None or 0} -- use 0 when reading the first file that contains\n",
    "                            header and None for all other files (default: {None})\n",
    "\n",
    "    Returns:\n",
    "        pandas DataFrame -- dataframe result from file\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file, names=col_names, header=header)\n",
    "\n",
    "\n",
    "def filename_components(x):\n",
    "    \"\"\" returns a series of tuples that unpacks the filepath name to its coponents\n",
    "\n",
    "    Arguments:\n",
    "        x {pahtlib.Path} -- a Path object\n",
    "\n",
    "    Returns:\n",
    "        tuple -- tuple of filename, file suffix, file prefix\n",
    "    \"\"\"\n",
    "    fpath = Path(x)\n",
    "    return (fpath.name, fpath.stem, fpath.suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## list of files\n",
    " using the glob library, this code will get a list of files that match a specified pattern\n",
    " these files should have been prepared using the split command as follows\n",
    "\n",
    " ```\n",
    " mkdir split\n",
    " split -l 100 -d --additional-suffix=.csv --suffix-length=5  lasso_monolayer_data_C33.csv \\\n",
    " ./split/lasso_monolayer_data_C33_\n",
    " wc -l split/*\n",
    " ```\n",
    "\n",
    " It is important that the outout filename includes  a sperator so that the numeric suffix is\n",
    " not confused with any numerics in the file name, in this example I used an underscore _ as\n",
    " the seperator. Output file name will be similar to:  lasso_monolayer_data_C33_00000.csv\n",
    " this ensures that we can extract the sequence number from the file name and use for indexing\n",
    " and determining the partial dataframes an find the first file in the list to extract the\n",
    " header names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = './data/ML_IE_C33/18M_uid_split/*.csv'\n",
    "# relative_path = './descriptors/split/*.csv'\n",
    "files_glob = glob.glob(relative_path)\n",
    "df_glob = pd.DataFrame(files_glob, columns=['fn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fcomponents_tuple_series = df_glob.fn.apply(filename_components)\n",
    "fcomponents_colns = fcomponents_tuple_series.apply(\n",
    "    pd.Series, index=['fname', 'stem', 'suffix'])\n",
    "# update df glob with filename components\n",
    "df_glob = pd.concat([df_glob, fcomponents_colns], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " so far the df_glob contains the following:\n",
    " fn: file path, to be used later to load files\n",
    " fname: file name only\n",
    " stem: the name of the file with out the extension\n",
    " suffix: the file extension\n",
    " example output for one entry:\n",
    "\n",
    " ```\n",
    " > df_glob.iloc[0]\n",
    " fn        ./descriptors/split/lasso_monolayer_data_C33_0...\n",
    " fname                    lasso_monolayer_data_C33_00006.csv\n",
    " stem                         lasso_monolayer_data_C33_00006\n",
    " suffix                                                 .csv\n",
    " Name: 0, dtype: object\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_glob.stem.str.split('^(?P<idx>\\d+)')\n",
    "# (?P<idx>\\d+)$\n",
    "# df_glob.stem.str.split('(\\d+)$').head() # this will split when it finds a number at the end\n",
    "\n",
    "# df_glob['sss'] = \"18M_as_as000\"\n",
    "# df_glob['sss'].str.split('(\\d+)').head() # this will split everytime it finds a number\n",
    "\n",
    "# what i need is not a split, it should be a search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This part extracts the file name from the stem column, file number represents the sequence\n",
    " of the partial file in the split set.\n",
    "\n",
    " `df_glob` is then indexed and sorted using the file number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a regex expression to extract the last sequence of digits in the file stem name\n",
    "df_glob['file_number'] = df_glob.stem.str.extract('(\\d+)$').astype(int)\n",
    "# index using the extracted file number and sort\n",
    "df_glob.set_index('file_number', inplace=True)\n",
    "df_glob.sort_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Read dataframe in parts\n",
    "\n",
    " the process invovles the following steps:\n",
    " 1. prepare an empty list to hold partial dataframes\n",
    " 2. read the first file in the list of splitted files, infer header since the first file already includes a header. \\\n",
    " append file to the list of data frames.\n",
    " 3. loop to read all remaining files in the list of splitted files, specify that the files dont include header, \\\n",
    " get header column names from the column names of the first dataframe, append part to the list of data frames\n",
    " 4. concat the list of dataframes to produce the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first file contains the header, rest of files have no header\n",
    "first_file = df_glob.fn[0]\n",
    "rest_of_files = df_glob.fn[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. read files without multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfs list is empty: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firsts file with header, number of rows = 249999\n",
      "subsequent reads will use the header from first df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:44<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to read files 45.63\n",
      "appended dataframes to list dfs1, size of list: 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare an empty list for storing the dataframes imported from files\n",
    "dfs1 = []\n",
    "print(f\"dfs list is empty: {len(dfs1)}\")\n",
    "\n",
    "# read and append the first file to the data frames list dfs\n",
    "timer1_read_start = time.time()\n",
    "dfs1.append(read_split(first_file, header=0))\n",
    "print(f\"first is file with header, number of rows = {dfs1[0].shape[0]}\")\n",
    "\n",
    "# using list comprehension to read the remaining splits, applying header form the first file\n",
    "print(\"subsequent reads will use the header from first df\")\n",
    "[dfs1.append(read_split(fn, dfs1[0].columns))\n",
    " for fn in tqdm.tqdm(rest_of_files)]\n",
    "\n",
    "timer1_read_end = time.time()\n",
    "timer1_read = timer1_read_end - timer1_read_start\n",
    "print(f\"time to read files {timer1_read:.2f}\")\n",
    "print(f\"appended dataframes to list dfs1, size of list: {len(dfs1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to concat dfs  45.63\n",
      "size of full df: 18834453\n"
     ]
    }
   ],
   "source": [
    "timer1_concat_start = time.time()\n",
    "full_df1 = pd.concat(dfs1, ignore_index=True)  # , names=dfs[0].columns)\n",
    "timer1_concat_end = time.time()\n",
    "timer1_concat = timer1_concat_end - timer1_concat_start\n",
    "print(f\"time to concat dfs  {timer1_read:.2f}\")\n",
    "print(f\"size of full df: {full_df1.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. read files WITH multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = multiprocessing.cpu_count()\n",
    "timer2_read_start = time.time()\n",
    "\n",
    "dfs2_first = read_split(first_file, header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:15<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dfs2_full shape = (18834453, 16)\n",
      "\n",
      "time to read and concat files 27.07\n"
     ]
    }
   ],
   "source": [
    "with concurrent.futures.ProcessPoolExecutor(cpu_count) as pool:\n",
    "    # [df2.append(read_split(fn, dfs2[0].columns)) for fn in rest_of_files]\n",
    "    # df['result'] = list(tqdm.tqdm(pool.map(func, df['a'], df['b'], chunksize=10), total=df.shape[0]))\n",
    "    # # With a progress bar\n",
    "\n",
    "    df_list = list(tqdm.tqdm(\n",
    "        pool.map(read_split, rest_of_files),\n",
    "        total=len(rest_of_files)))\n",
    "\n",
    "    df_combined = pd.concat(df_list)\n",
    "    df_combined.columns = dfs2_first.columns\n",
    "\n",
    "dfs2_full = dfs2_first.append(df_combined)\n",
    "\n",
    "\n",
    "timer2_read_end = time.time()\n",
    "timer2_read = timer2_read_end - timer2_read_start\n",
    "print(f'\\ndfs2_full shape = {dfs2_full.shape}')\n",
    "print(f'\\ntime to read and concat files {timer2_read:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. import as a single file using pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set index\n",
      "time to load 81.80\n"
     ]
    }
   ],
   "source": [
    "timer3_read_start = time.time()\n",
    "df3 = Data().get18M()\n",
    "timer3_read = time.time() - timer3_read_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal import 48.72\n",
      "parallel import 27.07\n",
      "pandas.read_csv 81.80\n"
     ]
    }
   ],
   "source": [
    "print(f'normal import {timer1_read + timer1_concat:.2f}')\n",
    "\n",
    "print(f'parallel import {timer2_read:.2f}')\n",
    "\n",
    "print(f'pandas.read_csv {timer3_read:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
